#!/usr/bin/env python3
"""
mix_and_resplit_yolo_pose.py 

Mix YOLO keypoint datasets from:
  • prepared_data_pose/yolo/train
  • prepared_data_pose/yolo/val
  • predicted_yolo_1
  • predicted_yolo_2
  • predicted_yolo_3
Then create a NEW randomized train/val split with unique filenames.

Result:
  out_dir/
    train/images/*.jpg|png
    train/labels/*.txt
    val/images/*.jpg|png
    val/labels/*.txt
    data.yaml
    manifest.csv   # mapping of src→dst

Usage:
  python mix_and_resplit_yolo_pose.py \
    --base_yolo prepared_data_pose/yolo \
    --extra predicted_yolo_1 predicted_yolo_2 predicted_yolo_3 \
    --out_dir yolo_mixed \
    --val_pct 0.12 --seed 42 --force
"""
import argparse, shutil, random, csv, hashlib
from pathlib import Path

IMG_EXTS = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}

NAMES = ["instrument"]
KPT_SHAPE = [5, 3]
FLIP_IDX  = [1, 0, 2, 4, 3]

def collect_pairs(img_dir: Path, lbl_dir: Path):
    pairs = []
    if not img_dir.exists() or not lbl_dir.exists():
        return pairs
    for p in sorted(img_dir.iterdir()):
        if p.suffix.lower() in IMG_EXTS:
            lbl = lbl_dir / (p.stem + ".txt")
            if lbl.exists() and lbl.stat().st_size > 0: 
                pairs.append((p, lbl))
    return pairs

def write_yaml(out_root: Path):
    yml = f"""# auto-generated by mix_and_resplit_yolo_pose.py
path: {out_root.resolve()}
train: train/images
val: val/images
names: {NAMES}
nc: {len(NAMES)}
kpt_shape: [{KPT_SHAPE[0]}, {KPT_SHAPE[1]}]
flip_idx: {FLIP_IDX}
"""
    (out_root / "data.yaml").write_text(yml)

def unique_stem(tag: str, img_path: Path, used: set):
    """Generate unique stem using tag, original filename, and hash to prevent collisions"""
    base = f"{tag}_{img_path.stem}_{hashlib.sha1(str(img_path).encode()).hexdigest()[:8]}"
    name = base
    bump = 2
    while name in used:
        name = f"{base}_{bump}"
        bump += 1
    used.add(name)
    return name

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--base_yolo", required=True, help="prepared_data_pose/yolo")
    ap.add_argument("--extra", nargs='+', required=True, 
                    help="one or more extra folders (e.g., predicted_yolo_1 predicted_yolo_2 predicted_yolo_3)")
    ap.add_argument("--out_dir", required=True, help="output folder for mixed dataset")
    ap.add_argument("--val_pct", type=float, default=0.10, help="validation fraction (0..1)")
    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--force", action="store_true", help="clear out_dir before writing")
    args = ap.parse_args()

    base = Path(args.base_yolo)
    extra_folders = [Path(folder) for folder in args.extra]
    out_root = Path(args.out_dir)

    if out_root.exists() and args.force:
        shutil.rmtree(out_root)
    for split in ["train", "val"]:
        (out_root / split / "images").mkdir(parents=True, exist_ok=True)
        (out_root / split / "labels").mkdir(parents=True, exist_ok=True)

    srcs = [
        (base / "train" / "images", base / "train" / "labels", "baseTr"),
        (base / "val"   / "images", base / "val"   / "labels", "baseVa"),
    ]
    
    # Add extra sources with unique tags based on folder name
    for i, extra_folder in enumerate(extra_folders, 1):
        folder_name = extra_folder.name
        tag = f"pred{i}_{folder_name}"  
        srcs.append((extra_folder / "images", extra_folder / "labels", tag))

    all_pairs = []
    counts = {}
    
    for imgd, lbld, tag in srcs:
        pairs = collect_pairs(imgd, lbld)
        counts[tag] = len(pairs)
        all_pairs.extend([(img, lbl, tag) for img, lbl in pairs])
        print(f"Found {len(pairs)} pairs in {tag} ({imgd})")

    assert all_pairs, "No (image,label) pairs found across the inputs."

    random.Random(args.seed).shuffle(all_pairs)
    n_total = len(all_pairs)
    n_val = max(1, int(n_total * args.val_pct))
    val_pairs = all_pairs[:n_val]
    train_pairs = all_pairs[n_val:]

    used_global = set()

    manifest_p = out_root / "manifest.csv"
    mf = open(manifest_p, "w", newline="", encoding="utf-8")
    mw = csv.writer(mf)
    mw.writerow(["src_tag", "src_image", "src_label", "dst_split", "dst_stem", "dst_image", "dst_label"])

    def dump(split_name, pairs):
        for img_p, lbl_p, tag in pairs:
            stem = unique_stem(tag, img_p, used_global)  # collision-safe stem using global set
            dst_img = out_root / split_name / "images" / (stem + img_p.suffix.lower())
            dst_lbl = out_root / split_name / "labels" / (stem + ".txt")
            shutil.copy2(img_p, dst_img)
            shutil.copy2(lbl_p, dst_lbl)
            mw.writerow([tag, str(img_p), str(lbl_p), split_name, stem, str(dst_img), str(dst_lbl)])

    dump("train", train_pairs)
    dump("val",   val_pairs)
    mf.close()

    # data.yaml for YOLOv11-pose
    write_yaml(out_root)

    # Print summary
    print(f"\nDataset Collection Summary:")
    for tag, count in counts.items():
        print(f"  {tag}: {count} pairs")
    print(f"\nTotal collected: {n_total} pairs")
    print(f"Split result: train={len(train_pairs)}, val={len(val_pairs)} ({args.val_pct*100:.1f}% val)")
    print(f"\nOutput written to: {out_root.resolve()}")
    print(f"Manifest file: {manifest_p.resolve()}")

if __name__ == "__main__":
    main()